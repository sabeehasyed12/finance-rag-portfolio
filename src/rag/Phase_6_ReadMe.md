# Phase 6 Retrieval Augmented Knowledge Base RAG

## Purpose

Phase 6 builds a grounded and auditable retrieval system over analytics documentation.

The goal is not chat.  
The goal is accurate answers backed by real source documents.

This phase answers questions like:

“How is churn calculated?”

Using only:

- documented metric definitions
- canonical SQL
- dbt model documentation
- governance rules
- incident runbooks

Every answer is explicitly tied to retrieved evidence.

---

## What Was Built

A fully local RAG system with:

- curated analytics documentation
- chunked and embedded knowledge base
- FAISS vector index
- deterministic retrieval and reranking
- grounded answer composition
- explicit source citations

No LLM is required to complete Phase 6.

---

## Knowledge Base Scope

The retrieval corpus lives in: docs/knowledge_base/


### Indexed document types

- **metric_definitions.md**  
  Authoritative business definitions and logic

- **example_sql.md**  
  Canonical SQL used by Gold and Platinum models

- **dbt_model_docs.md**  
  Model purpose, grain, and lineage

- **schema_descriptions.md**  
  Layer intent and guarantees

- **data_quality_rules.md**  
  Enforced assumptions and expectations

- **runbooks.md**  
  Incident diagnosis and remediation steps

### Explicit exclusions

- no dashboards
- no ad hoc queries
- no exploratory logic

Only approved and versioned knowledge is indexed.

---

## Chunking Strategy

### Why chunking matters

Analytics documentation is long, hierarchical, and repetitive.  
Naive chunking causes:

- intros to dominate retrieval
- unrelated sections to bleed together
- SQL to be buried under prose

Chunking is therefore carefully tuned.

### Chunk parameters

Implemented in `build_kb.py`:

- target length approximately 500 to 900 characters
- overlap approximately 80 to 120 characters
- minimum chunk length 80 characters

Small chunks are intentionally allowed to ensure:

- SQL examples are preserved
- short metric definitions are indexed
- sections remain isolated

### Header aware chunking

Each chunk preserves:

- section headers
- source file
- section name
- line numbers

This enables precise citations such as: docs/knowledge_base/example_sql.md | Churn Calculation | L12 to L38



---

## Embedding Model

### Model used

`all-MiniLM-L6-v2`

### Rationale

- fast
- CPU friendly
- strong semantic recall for technical documentation
- no external API dependency

Embeddings are normalized and stored as float32 vectors.

---

## Vector Index

### FAISS index
artifacts/faiss/index.faiss

### Chunk metadata
artifacts/faiss/chunks.jsonl


### Design choices

- local index for fast startup
- reproducible builds
- no hidden state
- fully rebuildable from source docs

---

## Retrieval Logic

Implemented in `ask_kb.py`.

### Step 1 Semantic search

- embed the user question
- retrieve top N similar chunks from FAISS

### Step 2 Keyword gating hybrid retrieval

For churn related questions:

- require the word `churn` to appear in chunk text

This prevents:

- lifecycle documentation leakage
- subscription noise
- unrelated runbooks matching on cancellation language

### Step 3 Intent aware reranking

Retrieval results are reranked with explicit signals.

**Penalties**

- intro and purpose sections for “how is X calculated” queries

**Bonuses**

- chunks containing SQL (`select` or fenced sql blocks)
- chunks from `example_sql.md`
- chunks from `metric_definitions.md`
- explicit “Churn Calculation” headers

This enforces:

- SQL over prose
- definitions over commentary
- formulas over explanations

### Step 4 Source deduplication

Only one chunk per source file is returned to ensure:

- breadth of evidence
- no repeated fragments from the same document

---

## Grounded Answer Composition

### No model generation

Answers are not generated by an LLM.

Instead:

- retrieved chunks are inspected
- domain signals are checked
- a deterministic answer is composed

Example logic:

```python
if "churn" and "subscription_status = canceled":
    answer = churn definition
```

## Output Format

When running:

```bash
python src/rag/ask_kb.py

```

## Output Details

The output includes:

- the question
- a grounded answer
- ranked retrieved sources with:
  - score
  - file
  - section
  - line numbers
  - text preview

This makes retrieval behavior transparent and debuggable.

---

## Key Design Nuances

### Intro sections are dangerous

Well written introductions match everything semantically.  
They must be penalized or they dominate retrieval.

### SQL is the strongest signal

For calculation questions:

- SQL beats explanations
- SQL beats definitions
- SQL beats runbooks

This priority is explicitly encoded.

### Hybrid retrieval beats pure embeddings

Semantic similarity alone is insufficient.  
Keyword gates and intent aware scoring are required.

### No LLM until retrieval is correct

Phase 6 proves:

- the correct evidence is found
- consistently

Before generation is introduced, preventing compounding errors later.







